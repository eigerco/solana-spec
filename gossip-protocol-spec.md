# Gossip protocol

Solana nodes use gossip protocol to communicate with each other. Communication is performed via UDP sockets. Each node can be started either in `gossip` or `spy` mode. In the first case it binds to a specified socket and fully participates in gossip. In the latter case it binds to a random UDP port from range 8000-10000 and spies on gossip via pull requests.

## Data management in gossip service

Managing of data is divided into several steps:

1. Data is received over UDP sockets - data is gathered in a batch and processed further
2. Consuming of the packets - data is deserialized, sanitized and verified before processing
3. Processing of the packets - based on the protocol type data is handled in a different way. The gossip protocol defines following types of messages:
* pull request
* pull response
* push message
* prune message
* ping message
* pong message
4. Gossip logic is running in the background thread - it generates push & pull messages and pings which are sent to peers. 
5. Sending responses over sockets. 

Each of the above steps run in separate threads. Communication between them is performed via `crossbeam-channel` crate. It provides a thread-safe multi-consumer multi-producer channels for message passing.

### Data protocol

Data sent in the packets is defined as:
``` rust
enum Protocol {
  PullRequest(CrdsFilter, CrdsValue),
  PullResponse(Pubkey, Vec<CrdsValue>),
  PushMessage(Pubkey, Vec<CrdsValue>),
  PruneMessage(Pubkey, PruneData),
  PingMessage(Ping),
  PongMessage(Pong)
}
```

where:
* `PullRequest` - is sent by node to ask the cluster for new information. It is sent to a single peer selected randomly and contains a bloom filter that represents things node already has. Pull requests contain two values:
  * a filter used for data filtering
  * a value, usually a `LegacyContactInfo` of the node who send the pull request containing nodes socket addresses for different protocols (gossip, tvu, tpu, rpc, etc.)
* `PullResponse` - these are sent in response to `PullRequest` and contain a public key of sending node and a list of new values. They are filtered and divided into 3 groups by receiving node:
  * responses that update owner timestamp
  * responses that don't update it
  * hash values of outdated values which failed to be inserted
Finally pull responses are inserted into `crds`.
* `PushMessage` - it is sent by nodes who want to share information with others. They contain a public key of sending node and list of values. Node receiving the message checks for:
    - duplication - duplicated messages are dropped, node response with prune message if message came from low staked node
    - new data:
        - new information is stored in `crds` and replaces old value
        - stores message in `pushed_once` which is used for detecting duplicates
        - retransmits information to its peers
    - expiration - messages older than `PUSH_MSG_TIMEOUT` are dropped
* `PruneMessage` - only messages that didnâ€™t time out and are destined to the node are processed - origin of the message is added into bloom filter and node drops it. Prune is an indication that there is another, higher stake weighted path to that node other than direct push (*improve this!*)
* `PingMessage` - nodes are sending ping messages from time to time to other nodes to check whether they are active. `PingMessage` contains a 32 bytes token generated by the origin.
* `PongMessage` - sent by node as a response to `PingMessage` - contains public key of sending node, its signature and hash of the token sent in `PingMessage`.

Values sent in messages contain signature and `CrdsData` which can be one of:
* `LegacyContactInfo`
* `Vote`
* `LowestSlot`
* `LegacySnapshotHashes`
* `AccountsHashes`
* `EpochSlots`
* `LegacyVersion`
* `Version`
* `NodeInstance`
* `DuplicateShred`
* `SnapshotHashes`
* `ContactInfo`
* `RestartLastVotedForkSlots`
* `RestartHeaviestFork`

#### `LegacyContactInfo`
Contains nodes public key, shred version, wallclock and socket addresses for different protocols:
* id - public key of origin
* gossip - gossip protocol address
* tvu - address to connect to for replication
* tvu quic - TVU over QUIC protocol
* serve repair quic - repair service for QUIC protocol
* tpu - transactions address
* tpu forwards - address to forward unprocessed replications
* tpu vote - address for bank state requests
* rpc - address for JSON-RPC requests
* rpc pubsub - websocket for JSON-RPC push notifications
* serve repair - address for send repair requests
* wallclock - timestamp of data creation
* shred_version - the shred version node has been configured to use

```rust
pub struct LegacyContactInfo {
    id: Pubkey,
    gossip: SocketAddr,
    tvu: SocketAddr,
    tvu_quic: SocketAddr,
    serve_repair_quic: SocketAddr,
    tpu: SocketAddr,
    tpu_forwards: SocketAddr,
    tpu_vote: SocketAddr,
    rpc: SocketAddr,
    rpc_pubsub: SocketAddr,
    serve_repair: SocketAddr,
    wallclock: u64,
    shred_version: u16,
}
```

#### `Vote`
Contains a one byte index and a `Vote` structure:
 - from - public key of origin
 - transaction - an atomically-committed sequence of instructions
 - wallclock - timestamp of data creation
 - slot - the unit of time givent to a leader for encoding a block, it is actually an `u64` type

 ```rust
 pub struct Vote {
    from: Pubkey,
    transaction: Transaction,
    wallclock: u64,
    slot: Option<Slot>,
}
 ```

#### `LowestSlot`
Contains a one byte index (deprecated) and a `LowestSlot` structure:
- from - public key of origin
- root - deprecated
- lowest - unit of time for encoding a block
- slots - deprecated
- stash - deprecated
- wallclock - timestamp of data creation

```rust
pub struct LowestSlot {
    from: Pubkey,
    root: Slot,
    lowest: Slot,
    slots: BTreeSet<Slot>,
    stash: Vec<deprecated::EpochIncompleteSlots>,
    wallclock: u64,
}
```

#### `LegacySnapshotHashes`, `AccountsHashes`
Contains:
- from - public key of origin
- hashes - a list of slots and hashes
- wallclock - timestamp of data creation

```rust
type LegacySnapshotHashes = AccountsHashes;
pub struct `AccountsHashes` {
    from: Pubkey,
    hashes: Vec<(Slot, Hash)>,
    wallclock: u64,
}
```

#### `EpochSlots`
Contains a one byte index and a `EpochSlots` structure:
- from - public key of origin
- slots - ??
- wallclock - timestamp of data creation

```rust
pub struct EpochSlots {
    from: Pubkey,
    slots: Vec<CompressedSlots>,
    wallclock: u64,
}
```

#### `LegacyVersion`
- from - public key of origin
- wallclock - timestamp of data creation
- version - older version of the Solana used earlier 1.3.x releases
```rust
pub struct LegacyVersion {
    from: Pubkey,
    wallclock: u64,
    version: solana_version::LegacyVersion1,
}
```

#### `Version`
- from - public key of origin
- wallclock - timestamp of data creation
- version - version of the Solana
```rust
pub struct Version {
    from: Pubkey,
    wallclock: u64,
    version: solana_version::LegacyVersion2,
}
```

#### `NodeInstance`
- from - public key of origin
- wallclock - timestamp of data creation
- timestamp - when the instance was created
- token - randomly generated value at node instantiation.
```rust
pub struct NodeInstance {
    from: Pubkey,
    wallclock: u64,
    timestamp: u64,
    token: u64,
}
```

#### `DuplicateShred`
Contains a 2 byte index and `DuplicateShred` structure:
- from - public key of origin
- wallclock - timestamp of data creation
- slot - unit of time for encoding a block
```rust
pub struct DuplicateShred {
    from: Pubkey,
    wallclock: u64,
    slot: Slot,
    _unused: u32,
    _unused_shred_type: ShredType,
    // Serialized DuplicateSlotProof split into chunks.
    num_chunks: u8,
    chunk_index: u8,
    chunk: Vec<u8>,
}
```

#### `SnapshotHashes`
- from - public key of origin
- full - 
- incremental - 
- wallclock - timestamp of data creation
```rust
pub struct SnapshotHashes {
    from: Pubkey,
    full: (Slot, Hash),
    incremental: Vec<(Slot, Hash)>,
    wallclock: u64,
}
```

#### `ContactInfo`
- pubkey - public key of origin
- wallclock - timestamp of data creation
- outset - 
- shred_version - the shred version node has been configured to use
- version - Solana version
- addrs - list of unique IP addresses
- sockets - list of nique sockets
- extensions - 
- cache - 
```rust
pub struct ContactInfo {
    pubkey: Pubkey,
    wallclock: u64,
    outset: u64,
    shred_version: u16,
    version: solana_version::Version,
    addrs: Vec<IpAddr>,
    sockets: Vec<SocketEntry>,
    extensions: Vec<Extension>,
    cache: [SocketAddr; SOCKET_CACHE_SIZE],
}
```

#### `RestartLastVotedForkSlots`
- from - public key of origin
- wallclock - timestamp of data creation
- offsets - 
- last_voted_slot - 
- last_voted_hash - 
- shred_version - the shred version node has been configured to use
```rust
pub struct RestartLastVotedForkSlots {
    from: Pubkey,
    wallclock: u64,
    offsets: SlotsOffsets,
    last_voted_slot: Slot,
    last_voted_hash: Hash,
    shred_version: u16,
}
```

#### `RestartHeaviestFork`
- from - public key of origin
- wallclock - timestamp of data creation
- last_slot - 
- last_hash - 
- observed_stake - 
- shred_version - the shred version node has been configured to use
```rust
pub struct RestartHeaviestFork {
    from: Pubkey,
    wallclock: u64,
    last_slot: Slot,
    last_slot_hash: Hash,
    observed_stake: u64,
    shred_version: u16,
}
```


### Receiving data 

Packet receiver thread binds to a socket on address 0.0.0.0 and specified port in case of running in `gossip` mode or on at random port in 8000-10000 range in case of `spy` mode. Packets are collected into a packet batch that is sent via `crossbeam-channel` for further processing. 

Each packet is sent in a binary form with a maximum size of 1232 bytes (1280 is a minimum `IPv6 TPU`, 40 bytes is the size of `IPv6` header and 8 bytes is the size of the fragment header). Apart from the data byte array packet contains additional metadata which contains:

 * size of the packet
 * address of the origin
 * port
 * flags, which can be one of:
    * DISCARD - `0b0000_0001`
    * FORWARDED - `0b0000_0010`
    * REPAIR - `0b0000_0100`
    * SIMPLE_VOTE_TX - `0b0000_1000`
    * TRACER_PACKET - `0b0001_0000`
    * ROUND_COMPUTE_UNIT_PRICE - `0b0010_0000`

### Consuming packets

Data packets are being checked before further processing. First packets are deserialized from the binary form into a `Protocol` type and then sanitized and verified.

#### Deserialization 

Packets are deserialized into a `Protocol` type defined above.

#### Data sanitization

Sanitization excludes signature-verification checks as these are performed in the next step. Sanitize check should include but are not limited to:

* all index values are in range
* all values are within their static min/max bounds

#### Data verification

Verification is handled differently according to type of the message:

* pull request - public key of the incoming value is verified,
* pull response, push message - each value from the incoming array is verified as above, only verified values are processed further
* prune message - public key of the incoming value is verified,
* ping - token is verified
* pong - hash of received ping token is verified

Only successfully verified packets are processed in the next step.

### Processing packets

### Gossip loop

The gossip loop runs in a separate thread. Each iteration the following steps are performed:

* before loop starts node send a push message containing `Version` and `NodeInstance`
* push messages are generated:
  * all entries from `crds` with timestamps inside current wallclock window are gathered 
  * for each entry nodes from active set are collected; pruned nodes are excluced unless entry should be pushed to the prunes too
  * list of push messages is created - each node will have a list of `crds` values associated
  * entries of below types will be dropped if their origins stake is below required value (currently 1 sol):
    * `LowestSlot`
    * `LegacyVersion`
    * `DuplicateShred`
    * `RestartHeaviestFork`
    * `RestartLastVotedForkSlots`
* pull requests are generated:
  * list of valid (shred version matches or equals 0) and active (updated `crds` within 60 seconds from now) gossip nodes is collected
  * for nodes which should be pinged (ones not pinged yet, or if cached pong is too old) a list of ping requests is created
  * from each gossip address only nodes with highest stake are kept in the list
  * for each node weight is calculated as follows:
  ```rust
  let stake = node_self_stake.min(node_stake[i]) / LAMPORTS_PER_SOL;
  let weight = u64::BITS - stake.leading_zeros();
  let weight = u64::from(weight).saturating_add(1).saturating_pow(2);
  weight
  ```
    where: `node_self_stake` - stake of "our" node, `node_stake[i]` - stake of node from the list,
  * `crds` filters are created from `crds` values, purged values and failed inserts
  * filters are divided among peers selected randomly using weights calculated above - the higher nodes weight, the more filters will be associated with it
  * additional randomly selected node which was not discovered yet is added to the list of nodes with all filters associated
  * a pull request list is created from a list of filters and nodes self `LegacyContactInfo`
  * pull requests are mapped into list of nodes
* push messages, pull requests and ping messages are sent
* values older than specified timeout are purged from `crds`
* old failed insterts are also purged (these are pull responses which failed to be inserted into `crds` - they are preserved to stop sender sending back the same outdated payload by adding them to the filter for next pull request)
* every 7500ms:
  * node sends a push requests containing the following data: `LegacyContactInfo`, `ContactInfo`, `NodeInstance`,
  * node refreshes its active set of nodes:
    * list of valid (shred version matches or equals 0) and active (updated crds within 60 seconds from now) gossip nodes is collected
    * for nodes which should be pinged (ones not pinged yet, or if cached pong is too old) * a list of ping requests is created
    * from each gossip address only nodes with highest stake are kept in the list
    * set of active nodes is rotated (*find out how!*)
    * pings are sent
